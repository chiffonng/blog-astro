---
title: (CMU Advanced NLP)
repo: https://github.com/chiffonng/cmu-advanced-nlp-minllama-assignment
context: personal
tags:
  - Large Language Model (LLM)
  - PyTorch
---

Developing a minimalist version of Llama2 by working with pure PyTorch and
pretrained weights `stories42M.pt` (an 8-layer, 42M parameter language model
pretrained on the [TinyStories] dataset). I learned about essential components
of an LLM (rotary position embedding (RoPE), scaled dot product attention,
`AdamW` optimizer) and sentence classification.
